# Goodhart's Law Simulation - Default Configuration
#
# To customize: copy to `config.toml` and edit (gitignored).
# CLI args override config values where applicable.

# =============================================================================
# CELL TYPES - The physics of the world
# =============================================================================
# Each cell type has intrinsic properties that define:
#   - energy_delta: Energy change when consumed (positive = food, negative = poison)
#   - interestingness: What proxy-metric agents observe (the misleading signal)
#   - color: RGB for visualization
#
# TODO (Goodhart Documentation):
#   Explain the design rationale for these interestingness values:
#   - Why is poison MORE interesting (1.0) than food (0.5)?
#   - How does this create an "anti-correlated" proxy metric?
#   - What real-world scenarios does this model? (engagement metrics, clickbait, etc.)
#   - Why is this a SHARPER demonstration than having similar values?
#   - What would happen if interestingness perfectly correlated with energy_delta?

[cell_types.empty]
color = [26, 26, 46]
energy_delta = 0.0
interestingness = 0.0

[cell_types.food]
color = [22, 199, 154]
energy_delta = 1.0
interestingness = 0.5   # Eat your vegetables, kids, or you'll kick yourself later

[cell_types.poison]
color = [255, 107, 107]
energy_delta = -2.0
interestingness = 1.0    # MORE interesting than food!

# =============================================================================
# WORLD - Grid geometry
# =============================================================================

[world]
width = 128
height = 128
# Note: World always uses toroidal wrapping (edges connect)

# =============================================================================
# RESOURCES - What spawns in the world
# =============================================================================

[resources]
food = 128
poison = 128
respawn = true           # Respawn consumed items

# =============================================================================
# AGENTS - Who lives in the world
# =============================================================================
# Each [[agents]] block spawns agents of that type.
# Types: OmniscientSeeker, ProxySeeker, or learned presets (ground_truth, proxy)

[[agents]]
type = "OmniscientSeeker"
count = 5

[[agents]]
type = "ProxySeeker"
count = 5

# Learned agent example:
# [[agents]]
# type = "ground_truth"
# count = 3
# model = "models/ground_truth.pth"

[agent]
view_range = 5           # Observation radius (creates 11x11 view)
energy_start = 1.0
energy_move_cost = 0.01  # Energy cost per move (direct value, no scaling)
move_cost_exponent = 1.5
max_move_distance = 1
death_penalty_ratio = 4.0  # Death penalty as multiple of food reward

# =============================================================================
# VISUALIZATION - main.py display settings
# =============================================================================

[visualization]
speed = 50               # Animation interval (ms), lower = faster
steps = 0                # 0 = infinite, N = stop after N steps

[brain_view]
enabled = false          # Single-agent mode for neural net visualization
agent_type = "ground_truth"
model = ""               # Empty = auto-detect latest
freeze_energy = true     # Keep energy constant for cleaner visualization

# =============================================================================
# TRAINING - PPO hyperparameters
# =============================================================================

[training]
# Algorithm
learning_rate = 0.0003
gamma = 0.99             # Discount factor
gae_lambda = 0.95        # GAE bias-variance tradeoff
eps_clip = 0.2           # PPO clipping (initial, decays to eps_clip_final)
eps_clip_final = 0.1     # Final clip (tighter trust region late in training)
k_epochs = 4             # PPO epochs per update
value_coef = 0.5

# Entropy scheduling: prevents premature collapse while allowing full convergence
# Two-phase approach:
#   Phase 1 (0-70%): entropy_coef decays, floor stays constant (stable learning)
#   Phase 2 (70-100%): entropy_coef at final, floor decays to 0 (allow determinism)
entropy_initial = 0.1    # Strong exploration early
entropy_final = 0.01     # Minimal when near-optimal
entropy_decay_fraction = 0.7  # Coef decay duration; floor decays AFTER this
entropy_floor = 0.5      # Min entropy during Phase 1 (log(8)/4)
entropy_floor_penalty = 0.05  # Penalty coefficient for floor violation

# Learning rate decay: reduces LR over training for fine-tuning
lr_decay = true          # Enable LR decay
lr_final = 0.0001       # Final LR (10x lower than initial 0.0003)

# Batching
n_envs = 192             # Parallel environments
n_minibatches = 1        # Full batch gradient descent (fits in VRAM)
steps_per_env = 128      # Rollout length (horizon)
steps_per_episode = 512  # Max steps before forced reset

# Architecture
brain_type = "base_cnn"
value_head_type = "popart"
popart_rescale_weights = true   # Rescale fc weights when stats change (required for correct values)
action_space_type = "discrete_grid"

# Environment curriculum
initial_food = 256
final_food = 64
curriculum_fraction = 0.7
min_food = 64
max_food = 256
min_poison = 64
max_poison = 256

# Performance
compile_models = true    # torch.compile policy (JIT)
compile_env = true       # torch.compile env.step() for better GPU utilization
compile_mode = "max-autotune"  # reduce-overhead, max-autotune, max-autotune-no-cudagraphs
use_amp = true           # Mixed precision (fp16)

# Checkpointing
checkpoint_interval = 0  # Save every N updates (0 = disabled)

# Privileged critic: value function sees episode density (food/poison counts)
# This improves value estimation without affecting policy behavior
privileged_critic = true

# Validation
validation_interval = 0  # Eval every N updates (0 = disabled)
validation_episodes = 16
validation_mode = "training"
validation_food = 128
validation_poison = 128

# =============================================================================
# RUNTIME - Device and system settings
# =============================================================================

[runtime]
# device = "cuda"        # Override auto-detection (or use GOODHARTS_DEVICE env)
# hsa_override_gfx_version = "11.0.0"  # ROCm iGPU workaround
cudnn_benchmark = true   # Algorithm autotuning (slow startup, faster training)
