# Goodhart's Law Simulation - Default Configuration
#
# To customize: copy to `config.toml` and edit (gitignored).
# CLI args override config values where applicable.

# =============================================================================
# CELL TYPES - The physics of the world
# =============================================================================
# Each cell type has intrinsic properties that define:
#   - energy_delta: Energy change when consumed (positive = food, negative = poison)
#   - interestingness: What proxy-metric agents observe (the misleading signal)
#   - color: RGB for visualization
#
# Interestingness Design Rationale
# --------------------------------
# Food is MORE interesting than poison (1.0 vs 0.5), yet proxy agents still
# die in droves. This is the key insight: the proxy metric isn't adversarial,
# it's just INCOMPLETE. Interestingness doesn't encode harm.
#
# Why this is a more honest Goodhart demonstration:
#   Real proxy metrics aren't designed to cause harm - they just fail to
#   capture everything that matters. Engagement metrics don't TRY to promote
#   outrage; they simply don't penalize it. Our proxy is the same: it rewards
#   "interesting" consumption without distinguishing helpful from harmful.
#
# The failure mechanism:
#   - Proxy agents learn to seek interestingness (correctly optimizing the proxy)
#   - Food is more interesting, so they DO prefer food when choosing
#   - But poison is ALSO interesting (0.5 > 0), so they eat it too
#   - The metric gives no signal that poison is deadly
#   - Result: agents optimize correctly and still die
#
# Real-world parallels:
#   - Engagement metrics reward clicks without encoding "was this helpful?"
#   - Citation counts reward attention without encoding "was this true?"
#   - Revenue metrics reward sales without encoding "did this help the user?"
#   In each case, the proxy captures SOMETHING we want, but misses a crucial
#   dimension (helpfulness, truth, user welfare).
#
# The counterfactual:
#   If interestingness perfectly matched energy_delta (food=1.0, poison=-2.0),
#   there would be no Goodhart failure - the proxy would BE the ground truth.
#   This is the impossible ideal we wish we had for real-world metrics.

[cell_types.empty]
color = [26, 26, 46]
energy_delta = 0.0
interestingness = 0.0

[cell_types.food]
color = [22, 199, 154]
energy_delta = 1.0
interestingness = 1.0   # Food is MORE interesting than poison...

[cell_types.poison]
color = [255, 107, 107]
energy_delta = -2.0
interestingness = 0.5   # ...yet agents STILL eat poison, because interestingness doesn't encode harm

# =============================================================================
# WORLD - Grid geometry
# =============================================================================

[world]
width = 128
height = 128
# Note: World always uses toroidal wrapping (edges connect)

# =============================================================================
# RESOURCES - What spawns in the world
# =============================================================================

[resources]
food = 128
poison = 128
respawn = true           # Respawn consumed items

# =============================================================================
# AGENTS - Who lives in the world
# =============================================================================
# Each [[agents]] block spawns agents of that type.
# Types: OmniscientSeeker, ProxySeeker, or learned presets (ground_truth, proxy)

[[agents]]
type = "OmniscientSeeker"
count = 5

[[agents]]
type = "ProxySeeker"
count = 5

# Learned agent example:
# [[agents]]
# type = "ground_truth"
# count = 3
# model = "models/ground_truth.pth"

[agent]
view_range = 5           # Observation radius (creates 11x11 view)
energy_start = 1.0
energy_move_cost = 0.01  # Energy cost per move (direct value, no scaling)
move_cost_exponent = 1.5 # Semi-stub, for moving more than one space at a time
max_move_distance = 1
death_penalty_ratio = 4.0  # Death penalty as multiple of food reward

# =============================================================================
# VISUALIZATION - main.py display settings
# =============================================================================

[visualization]
speed = 50               # Animation interval (ms), lower = faster
steps = 0                # 0 = infinite, N = stop after N steps

[brain_view]
enabled = false          # Single-agent mode for neural net visualization
agent_type = "ground_truth"
model = ""               # Empty = auto-detect latest
freeze_energy = true     # Keep energy constant for cleaner visualization

# =============================================================================
# TRAINING - PPO hyperparameters
# =============================================================================

[training]
# Algorithm
learning_rate = 0.0003
gamma = 0.99             # Discount factor
gae_lambda = 0.95        # GAE bias-variance tradeoff
eps_clip = 0.2           # PPO clipping (initial, decays to eps_clip_final)
eps_clip_final = 0.1     # Final clip (tighter trust region late in training)
k_epochs = 4             # PPO epochs per update
value_coef = 0.5

# Entropy scheduling: prevents premature collapse while allowing full convergence
# Two-phase approach:
#   Phase 1 (0-100%): entropy_coef decays, floor stays constant (stable learning)
#   Phase 2 (UNUSED): entropy_coef at final, floor decays to 0 (allow determinism)
entropy_initial = 0.1    # Strong exploration early
entropy_final = 0.001     # Minimal when near-optimal
entropy_decay_fraction = 1.0  # Coef decay duration; floor decays AFTER this
entropy_floor = 0.5      # Min entropy during Phase 1 (log(8)/4)
entropy_floor_penalty = 0.05  # Penalty coefficient for floor violation

# Learning rate decay: reduces LR over training for fine-tuning
lr_decay = true          # Enable LR decay
lr_final = 0.0001       # Final LR (10x lower than initial 0.0003)

# Batching
n_envs = 192             # Parallel environments
n_minibatches = 1        # Full batch gradient descent (fits in VRAM)
steps_per_env = 128      # Rollout length (horizon)
steps_per_episode = 512  # Max steps before forced reset

# Architecture
brain_type = "base_cnn"
value_head_type = "popart"
popart_beta_min = 0.001         # Min EMA decay for PopArt (smaller = slower adaptation, more stable)
popart_rescale_weights = true   # Rescale fc weights when stats change (required for correct values)
action_space_type = "discrete_grid"

# Environment randomization: each episode samples food/poison counts uniformly
# This prevents overfitting to specific densities and improves generalization
min_food = 64
max_food = 256
min_poison = 64
max_poison = 256

# Performance
compile_models = true    # torch.compile policy (JIT)
compile_env = true       # torch.compile env.step() for better GPU utilization
compile_mode = "max-autotune"  # reduce-overhead, max-autotune, max-autotune-no-cudagraphs
use_amp = true           # Mixed precision (fp16)

# Checkpointing
checkpoint_interval = 0  # Save every N updates (0 = disabled)

# Privileged critic: value function sees episode density (food/poison counts)
# This improves value estimation without affecting policy behavior
privileged_critic = true

# Validation
validation_interval = 0  # Eval every N updates (0 = disabled)
validation_episodes = 16
validation_mode = "training"
validation_food = 128
validation_poison = 128

# =============================================================================
# EVALUATION - Testing trained models
# =============================================================================

[evaluation]
n_envs = 8192            # Parallel environments (high = better GPU utilization)
steps_per_env = 16384     # Steps per environment
runs = 3                 # Runs per mode for statistical aggregation
base_seed = 42           # Starting seed (incremented per run)

# =============================================================================
# RUNTIME - Device and system settings
# =============================================================================

[runtime]
# device = "cuda"        # Override auto-detection (or use GOODHARTS_DEVICE env)
# hsa_override_gfx_version = "11.0.0"  # ROCm iGPU workaround
cudnn_benchmark = true   # Algorithm autotuning (slow startup, faster training)
