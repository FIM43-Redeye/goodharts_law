# Goodhart's Law Simulation - Default Configuration
#
# This file contains the default settings shipped with the project.
# To customize, copy this to `config.toml` and edit that file.
# config.toml is in .gitignore so your changes won't be committed.
#
# Usage:
#   python main.py                     # Uses config.toml (or config.default.toml)
#   python main.py --config other.toml # Uses specific config

# =============================================================================
# WORLD
# =============================================================================
[world]
width = 100
height = 100
loop = true   # true = edges wrap (toroidal), false = bounded with walls

# =============================================================================
# RESOURCES
# =============================================================================
[resources]
food = 500      # Matches training final_food for consistency
poison = 50     # Initial poison items
respawn = true  # Respawn consumed resources

# =============================================================================
# AGENTS
# =============================================================================
# Each [[agents]] block spawns agents of that type.
# Available types: OmniscientSeeker, ProxySeeker, 
#                  ground_truth, proxy, proxy_ill_adjusted (presets)

[[agents]]
type = "OmniscientSeeker"
count = 5

[[agents]]
type = "ProxySeeker" 
count = 5

# Example: learned agents
# [[agents]]
# type = "ground_truth"
# count = 3
# model = "goodharts/models/ground_truth.pth"

# =============================================================================
# AGENT PROPERTIES
# =============================================================================
[agent]
view_range = 5         # How far agents can see
energy_start = 50.0    # Starting energy
energy_move_cost = 0.01 # Cheap movement to encourage exploration (was 0.1)
move_cost_exponent = 1.5
max_move_distance = 3  # Speed cap

# =============================================================================
# VISUALIZATION (for main.py)
# =============================================================================
[visualization]
speed = 50    # Animation interval in ms (lower = faster)
steps = 0     # 0 = run forever, >0 = stop after N steps

# =============================================================================
# BRAIN VIEW MODE
# =============================================================================
# When enabled, spawns a SINGLE agent for neural network visualization.
# This REPLACES the [[agents]] list above.
[brain_view]
enabled = false
agent_type = "ground_truth"
model = ""           # Optional: specific model path (empty = auto-detect)
freeze_energy = true # If true, agent energy stays constant (for visualization)

# =============================================================================
# RUNTIME
# =============================================================================
[runtime]
# device = "cuda"      # Force specific device: "cpu", "cuda", "cuda:0", etc.
#                      # Leave commented for auto-detection (CUDA > CPU)
#                      # Can also use GOODHARTS_DEVICE env var

# =============================================================================
# TRAINING (for train_ppo.py and other training scripts)
# =============================================================================
[training]
# Curriculum learning - gradually make environment harder
initial_food = 200          # Start sparse (was 500)
final_food = 50             # End very sparse (was 200)
curriculum_fraction = 0.7   # Fraction of episodes for curriculum

# Environment
poison_count = 50           # Fixed poison count during training
steps_per_episode = 500     # Max steps per episode

# PPO hyperparameters
learning_rate = 0.001       # 1e-3 (Faster convergence)
gamma = 0.99                # Discount factor
eps_clip = 0.2              # PPO clipping parameter
k_epochs = 4                # PPO epochs per update
steps_per_env = 128       # Horizon length for PPO (critical for GAE)

# Reward shaping - provides gradient toward food/away from poison
reward_scale = 0.1          # Standardize rewards (was 1.0)
entropy_coef = 0.02         # Increased to prevent collapse with scaled rewards
shaping_food_attract = 0.5  # Reward weight for moving toward visible food
shaping_poison_repel = 0.3  # Reward weight for moving away from visible poison
enable_shaping = true       # Enable/disable shaping rewards
