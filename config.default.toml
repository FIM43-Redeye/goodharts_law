# Goodhart's Law Simulation - Default Configuration
#
# This file contains the default settings shipped with the project.
# To customize, copy this to `config.toml` and edit that file.
# config.toml is in .gitignore so your changes won't be committed.
#
# Usage:
#   python main.py                     # Uses config.toml (or config.default.toml)
#   python main.py --config other.toml # Uses specific config

# =============================================================================
# WORLD
# =============================================================================
[world]
width = 100
height = 100
loop = false  # true = edges wrap (toroidal), false = bounded with walls

# =============================================================================
# RESOURCES
# =============================================================================
[resources]
food = 500      # Matches training final_food for consistency
poison = 10     # Initial poison items
respawn = true  # Respawn consumed resources

# =============================================================================
# AGENTS
# =============================================================================
# Each [[agents]] block spawns agents of that type.
# Available types: OmniscientSeeker, ProxySeeker, 
#                  LearnedGroundTruth, LearnedProxy, LearnedProxyIllAdjusted

[[agents]]
type = "OmniscientSeeker"
count = 5

[[agents]]
type = "ProxySeeker" 
count = 5

# Example: learned agents
# [[agents]]
# type = "LearnedGroundTruth"
# count = 3
# model = "goodharts/models/ground_truth.pth"

# =============================================================================
# AGENT PROPERTIES
# =============================================================================
[agent]
view_range = 5         # How far agents can see
energy_start = 50.0    # Starting energy
energy_move_cost = 0.1 # Cost per unit distance
move_cost_exponent = 1.5
max_move_distance = 3  # Speed cap

# =============================================================================
# VISUALIZATION (for main.py)
# =============================================================================
[visualization]
speed = 50    # Animation interval in ms (lower = faster)
steps = 0     # 0 = run forever, >0 = stop after N steps

# =============================================================================
# BRAIN VIEW MODE
# =============================================================================
# When enabled, spawns a SINGLE agent for neural network visualization.
# This REPLACES the [[agents]] list above.
[brain_view]
enabled = false
agent_type = "LearnedGroundTruth"
model = ""           # Optional: specific model path (empty = auto-detect)
freeze_energy = true # If true, agent energy stays constant (for visualization)

# =============================================================================
# RUNTIME
# =============================================================================
[runtime]
# device = "cuda"      # Force specific device: "cpu", "cuda", "cuda:0", etc.
#                      # Leave commented for auto-detection (CUDA > CPU)
#                      # Can also use GOODHARTS_DEVICE env var

# =============================================================================
# TRAINING (for train_ppo.py and other training scripts)
# =============================================================================
[training]
# Curriculum learning - gradually make environment harder
initial_food = 500          # Start moderate (was 2500 - too trivial)
final_food = 200            # End sparse but learnable
curriculum_fraction = 0.7   # Fraction of episodes for curriculum

# Environment
poison_count = 30           # Fixed poison count during training
steps_per_episode = 500     # Max steps per episode

# PPO hyperparameters
learning_rate = 0.0003      # 3e-4
gamma = 0.99                # Discount factor
eps_clip = 0.2              # PPO clipping parameter
k_epochs = 4                # PPO epochs per update
update_timestep = 500       # Steps between PPO updates

# Reward shaping - provides gradient toward food/away from poison
reward_scale = 10.0         # Multiply base rewards by this factor
entropy_coef = 0.0          # Entropy bonus (0 = let policy specialize)
shaping_food_attract = 0.5  # Reward weight for moving toward visible food
shaping_poison_repel = 0.3  # Reward weight for moving away from visible poison
enable_shaping = true       # Enable/disable shaping rewards
