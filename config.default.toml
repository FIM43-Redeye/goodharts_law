# Goodhart's Law Simulation - Default Configuration
#
# This file contains the default settings shipped with the project.
# To customize, copy this to `config.toml` and edit that file.
# config.toml is in .gitignore so your changes won't be committed.
#
# Usage:
#   python main.py                     # Uses config.toml (or config.default.toml)
#   python main.py --config other.toml # Uses specific config

# =============================================================================
# WORLD
# =============================================================================
[world]
width = 100
height = 100
loop = true   # true = edges wrap (toroidal), false = bounded with walls

# =============================================================================
# RESOURCES
# =============================================================================
[resources]
food = 500      # Matches training final_food for consistency
poison = 50     # Initial poison items
respawn = true  # Respawn consumed resources

# =============================================================================
# AGENTS
# =============================================================================
# Each [[agents]] block spawns agents of that type.
# Available types: OmniscientSeeker, ProxySeeker, 
#                  ground_truth, proxy, proxy_jammed (presets)

[[agents]]
type = "OmniscientSeeker"
count = 5

[[agents]]
type = "ProxySeeker" 
count = 5

# Example: learned agents
# [[agents]]
# type = "ground_truth"
# count = 3
# model = "goodharts/models/ground_truth.pth"

# =============================================================================
# AGENT PROPERTIES
# =============================================================================
[agent]
view_range = 5         # How far agents can see
energy_start = 50.0    # Starting energy
energy_move_cost = 0.05 # Cheap movement to encourage exploration (was 0.1)
move_cost_exponent = 1.5
max_move_distance = 3  # Speed cap

# =============================================================================
# VISUALIZATION (for main.py)
# =============================================================================
[visualization]
speed = 50    # Animation interval in ms (lower = faster)
steps = 0     # 0 = run forever, >0 = stop after N steps

# =============================================================================
# BRAIN VIEW MODE
# =============================================================================
# When enabled, spawns a SINGLE agent for neural network visualization.
# This REPLACES the [[agents]] list above.
[brain_view]
enabled = false
agent_type = "ground_truth"
model = ""           # Optional: specific model path (empty = auto-detect)
freeze_energy = true # If true, agent energy stays constant (for visualization)

# =============================================================================
# RUNTIME
# =============================================================================
[runtime]
# device = "cuda"      # Force specific device: "cpu", "cuda", "cuda:0", etc.
#                      # Leave commented for auto-detection (CUDA > CPU)
#                      # Can also use GOODHARTS_DEVICE env var

# hsa_override_gfx_version = "11.0.0" # Override for ROCm iGPU support (e.g. gfx1103 -> 11.0.0)


# =============================================================================
# TRAINING (for train_ppo.py and other training scripts)
# =============================================================================
[training]
# Curriculum learning - gradually make environment harder
initial_food = 200          # Start sparse (was 500)
final_food = 50             # End very sparse (was 200)
curriculum_fraction = 0.7   # Fraction of episodes for curriculum

# Environment density ranges (randomized each update for robustness)
min_food = 50               # Minimum food count during training
max_food = 200              # Maximum food count during training  
min_poison = 20             # Minimum poison count during training
max_poison = 100            # Maximum poison count during training
steps_per_episode = 512     # Max steps per episode

# PPO hyperparameters
learning_rate = 0.0003       # 3e-4
gamma = 0.99                # Discount factor
gae_lambda = 0.95           # GAE lambda (bias-variance tradeoff)
eps_clip = 0.2              # PPO clipping parameter
k_epochs = 4                # PPO epochs per update
steps_per_env = 128         # Horizon length for PPO (critical for GAE)
n_envs = 64                 # Number of parallel environments
n_minibatches = 4           # Minibatches per epoch (fewer = faster but more VRAM)
value_coef = 0.5            # Value loss coefficient
brain_type = "base_cnn"     # Neural network architecture

# Reward shaping - provides gradient toward food/away from poison
reward_scale = 0.1          # Standardize rewards (was 1.0)
entropy_coef = 0.01         # Standard PPO entropy (was 0.002)
shaping_food_attract = 0.5  # Reward weight for moving toward visible food
shaping_poison_repel = 0.3  # Reward weight for moving away from visible poison
enable_shaping = true       # Enable/disable shaping rewards

# Performance
compile_models = true       # Enable torch.compile (JIT)
use_amp = true              # Enable Automatic Mixed Precision (float16)

# Long training run options
checkpoint_interval = 0     # Save checkpoint every N updates (0 = disabled)
lr_decay = false            # Enable learning rate decay over training
lr_decay_factor = 0.1       # Final LR = initial_lr * factor (if lr_decay enabled)
