# Goodhart's Law Simulation - Default Configuration
#
# To customize: copy to `config.toml` and edit (gitignored).
# CLI args override config values where applicable.

# =============================================================================
# CELL TYPES - The physics of the world
# =============================================================================
# Each cell type has intrinsic properties that define:
#   - energy_delta: Energy change when consumed (positive = food, negative = poison)
#   - interestingness: What proxy-metric agents observe (the misleading signal)
#   - color: RGB for visualization

[cell_types.empty]
color = [26, 26, 46]
energy_delta = 0.0
interestingness = 0.0

[cell_types.wall]
color = [74, 74, 74]
energy_delta = 0.0
interestingness = 0.0

[cell_types.food]
color = [22, 199, 154]
energy_delta = 1.0
interestingness = 1.0

[cell_types.poison]
color = [255, 107, 107]
energy_delta = -2.0
interestingness = 0.9    # Nearly as "interesting" as food - the trap!

[cell_types.prey]
color = [0, 255, 255]
energy_delta = 10.0     # Edible?
interestingness = 0.0

[cell_types.predator]
color = [255, 0, 0]
energy_delta = 0.0
interestingness = 1.0

# =============================================================================
# WORLD - Grid geometry
# =============================================================================

[world]
width = 128
height = 128
loop = true              # Toroidal (wrap edges) vs bounded (walls at edges)

# =============================================================================
# RESOURCES - What spawns in the world
# =============================================================================

[resources]
food = 128
poison = 128
respawn = true           # Respawn consumed items

# =============================================================================
# AGENTS - Who lives in the world
# =============================================================================
# Each [[agents]] block spawns agents of that type.
# Types: OmniscientSeeker, ProxySeeker, or learned presets (ground_truth, proxy)

[[agents]]
type = "OmniscientSeeker"
count = 5

[[agents]]
type = "ProxySeeker"
count = 5

# Learned agent example:
# [[agents]]
# type = "ground_truth"
# count = 3
# model = "models/ground_truth.pth"

[agent]
view_range = 5           # Observation radius (creates 11x11 view)
energy_start = 1.0
energy_move_cost = 0.01  # Energy cost per move (direct value, no scaling)
move_cost_exponent = 1.5
max_move_distance = 1
death_penalty_ratio = 4.0  # Death penalty as multiple of food reward

# =============================================================================
# VISUALIZATION - main.py display settings
# =============================================================================

[visualization]
speed = 50               # Animation interval (ms), lower = faster
steps = 0                # 0 = infinite, N = stop after N steps

[brain_view]
enabled = false          # Single-agent mode for neural net visualization
agent_type = "ground_truth"
model = ""               # Empty = auto-detect latest
freeze_energy = true     # Keep energy constant for cleaner visualization

# =============================================================================
# TRAINING - PPO hyperparameters
# =============================================================================

[training]
# Algorithm
learning_rate = 0.0003
gamma = 0.99             # Discount factor
gae_lambda = 0.95        # GAE bias-variance tradeoff
eps_clip = 0.2           # PPO clipping
k_epochs = 4             # PPO epochs per update
entropy_coef = 0.001
value_coef = 0.5

# Batching
n_envs = 192             # Parallel environments
n_minibatches = 1        # 1 = full batch gradient descent
steps_per_env = 128      # Rollout length (horizon)
steps_per_episode = 512  # Max steps before forced reset

# Architecture
brain_type = "base_cnn"
value_head_type = "popart"
action_space_type = "discrete_grid"

# Environment curriculum
initial_food = 256
final_food = 64
curriculum_fraction = 0.7
min_food = 64
max_food = 256
min_poison = 64
max_poison = 256

# Performance
compile_models = true    # torch.compile (JIT)
compile_mode = "max-autotune-no-cudagraphs"  # reduce-overhead, max-autotune, max-autotune-no-cudagraphs
use_amp = true           # Mixed precision (fp16)

# Checkpointing
checkpoint_interval = 0  # Save every N updates (0 = disabled)
lr_decay = false
lr_decay_factor = 0.1

# Validation
validation_interval = 0  # Eval every N updates (0 = disabled)
validation_episodes = 16
validation_mode = "training"
validation_food = 128
validation_poison = 128

# =============================================================================
# RUNTIME - Device and system settings
# =============================================================================

[runtime]
# device = "cuda"        # Override auto-detection (or use GOODHARTS_DEVICE env)
# hsa_override_gfx_version = "11.0.0"  # ROCm iGPU workaround
cudnn_benchmark = true   # Algorithm autotuning (slow startup, faster training)
