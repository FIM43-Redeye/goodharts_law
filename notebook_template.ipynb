{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodhart's Law Simulation - Colab Training\n",
    "\n",
    "This notebook trains RL agents to demonstrate Goodhart's Law: when a measure becomes a target, it ceases to be a good measure.\n",
    "\n",
    "**Setup:** Copy the entire `goodharts_law` folder to your Google Drive before running.\n",
    "\n",
    "**Expected Drive structure:**\n",
    "```\n",
    "My Drive/\n",
    "  goodharts_law/\n",
    "    goodharts/           <- Python package\n",
    "    config.default.toml  <- Configuration\n",
    "    models/              <- Created during training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Mount Drive, configure paths, and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure project path - EDIT THIS if your folder is named differently\n",
    "PROJECT_PATH = '/content/drive/MyDrive/goodharts_law'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Verify the path exists\n",
    "if not os.path.exists(PROJECT_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Project not found at {PROJECT_PATH}\\n\"\n",
    "        f\"Please copy the goodharts_law folder to your Google Drive.\"\n",
    "    )\n",
    "\n",
    "# Add to Python path so imports work\n",
    "if PROJECT_PATH not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "# Change to project directory (for config file loading)\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Memory info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")\n",
    "    print(\"Go to Runtime -> Change runtime type -> GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies if needed\n",
    "# (Colab has torch, numpy, matplotlib pre-installed)\n",
    "\n",
    "# Check for tomllib (Python 3.11+) or install toml\n",
    "try:\n",
    "    import tomllib\n",
    "except ImportError:\n",
    "    !pip install toml -q\n",
    "    print(\"Installed toml package\")\n",
    "\n",
    "# Verify package imports\n",
    "try:\n",
    "    from goodharts.utils.device import get_device\n",
    "    from goodharts.config import get_config\n",
    "    from goodharts.modes import get_all_mode_names\n",
    "    from goodharts.configs.default_config import get_simulation_config\n",
    "    \n",
    "    device = get_device()\n",
    "    config = get_config()\n",
    "    sim_config = get_simulation_config()\n",
    "    \n",
    "    print(f\"\\nPackage loaded successfully!\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"World size: {config.get('WORLD_SIZE')}\")\n",
    "    print(f\"Available training modes: {get_all_mode_names(sim_config)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"\\nCheck that the goodharts/ folder is in your Drive.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training Configuration\n",
    "\n",
    "Set your training parameters here. The defaults are tuned for Colab's free tier (T4 GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# These override config.default.toml settings\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    # Which mode(s) to train\n",
    "    # Options: 'ground_truth', 'ground_truth_handhold', 'ground_truth_blinded', 'proxy', 'all'\n",
    "    'mode': 'ground_truth',\n",
    "    \n",
    "    # Training duration\n",
    "    'total_timesteps': 500_000,  # Increase for better results (1M+ recommended)\n",
    "    \n",
    "    # Performance tuning for Colab\n",
    "    'n_envs': 128,           # Parallel environments (reduce if OOM)\n",
    "    'n_minibatches': 4,      # Increase if OOM (trades memory for speed)\n",
    "    \n",
    "    # Compilation (faster training after warmup)\n",
    "    'compile_models': True,  # Set False for faster startup, slower training\n",
    "    'use_amp': True,         # Mixed precision (faster, less memory)\n",
    "    \n",
    "    # Logging\n",
    "    'tensorboard': True,     # Enable TensorBoard logging\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Run Training\n",
    "\n",
    "Execute training with the configuration above. Progress is printed inline.\n",
    "\n",
    "**Tips:**\n",
    "- Training saves checkpoints to `models/` in your Drive\n",
    "- You can interrupt (stop button) and resume later - models are saved periodically\n",
    "- For the full Goodhart demonstration, train both `ground_truth` and `proxy` modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training components\n",
    "from goodharts.training.ppo import PPOTrainer, PPOConfig\n",
    "from goodharts.modes import get_all_mode_names\n",
    "from goodharts.configs.default_config import get_simulation_config\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Parse mode(s)\n",
    "sim_config = get_simulation_config()\n",
    "all_modes = get_all_mode_names(sim_config)\n",
    "\n",
    "mode_setting = TRAINING_CONFIG['mode']\n",
    "if mode_setting == 'all':\n",
    "    modes_to_train = all_modes\n",
    "elif ',' in mode_setting:\n",
    "    modes_to_train = [m.strip() for m in mode_setting.split(',')]\n",
    "else:\n",
    "    modes_to_train = [mode_setting]\n",
    "\n",
    "print(f\"Will train: {modes_to_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "results = {}\n",
    "\n",
    "for mode in modes_to_train:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training mode: {mode}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build config for this mode\n",
    "    output_path = f'models/ppo_{mode}.pth'\n",
    "    \n",
    "    ppo_config = PPOConfig.from_config(\n",
    "        mode=mode,\n",
    "        total_timesteps=TRAINING_CONFIG['total_timesteps'],\n",
    "        n_envs=TRAINING_CONFIG['n_envs'],\n",
    "        n_minibatches=TRAINING_CONFIG['n_minibatches'],\n",
    "        compile_models=TRAINING_CONFIG['compile_models'],\n",
    "        use_amp=TRAINING_CONFIG['use_amp'],\n",
    "        tensorboard=TRAINING_CONFIG['tensorboard'],\n",
    "        output_path=output_path,\n",
    "    )\n",
    "    \n",
    "    # Create trainer and run\n",
    "    trainer = PPOTrainer(ppo_config)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = trainer.train()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results[mode] = result\n",
    "    \n",
    "    print(f\"\\nCompleted {mode} in {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Model saved to: {output_path}\")\n",
    "    \n",
    "    # Clean up GPU memory between modes\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TensorBoard\n",
    "\n",
    "View training metrics in TensorBoard. Logs are saved to `generated/logs/tensorboard/`.\n",
    "\n",
    "**Metrics tracked:**\n",
    "- `loss/policy`, `loss/value` - PPO losses\n",
    "- `metrics/entropy` - Policy entropy (exploration)\n",
    "- `metrics/food_ratio` - food / (food + poison) per update\n",
    "- `reward/episode` - Episode rewards\n",
    "- `validation/*` - Validation metrics (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for TensorBoard logs\n",
    "import os\n",
    "import glob\n",
    "\n",
    "tb_dir = 'generated/logs/tensorboard'\n",
    "if os.path.exists(tb_dir):\n",
    "    subdirs = [d for d in os.listdir(tb_dir) if os.path.isdir(os.path.join(tb_dir, d))]\n",
    "    print(f\"TensorBoard logs found for: {subdirs}\")\n",
    "    \n",
    "    # Show event file counts\n",
    "    for subdir in subdirs:\n",
    "        events = glob.glob(os.path.join(tb_dir, subdir, 'events.*'))\n",
    "        print(f\"  {subdir}: {len(events)} event file(s)\")\n",
    "else:\n",
    "    print(f\"No TensorBoard logs found at {tb_dir}\")\n",
    "    print(\"Run training with tensorboard=True first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard (embedded in notebook)\n",
    "# This will show training curves for all modes\n",
    "%tensorboard --logdir generated/logs/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation\n",
    "\n",
    "Evaluate trained models using the continuous survival paradigm. Agents run until they die (starvation), then respawn. We track death events and survival times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available trained models\n",
    "import glob\n",
    "\n",
    "model_files = glob.glob('models/*.pth')\n",
    "print(\"Available models:\")\n",
    "for f in sorted(model_files):\n",
    "    size_mb = os.path.getsize(f) / 1024 / 1024\n",
    "    print(f\"  {f} ({size_mb:.1f} MB)\")\n",
    "\n",
    "if not model_files:\n",
    "    print(\"  No models found. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "EVAL_CONFIG = {\n",
    "    'models_to_evaluate': ['ground_truth', 'proxy'],  # Edit this list\n",
    "    'total_timesteps': 1000,   # Steps per environment\n",
    "    'n_envs': 64,              # Parallel environments\n",
    "    'deterministic': False,    # Use stochastic policy (more realistic)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation using ModelTester (continuous survival paradigm)\n",
    "from goodharts.evaluation.evaluator import ModelTester, EvaluationConfig\n",
    "\n",
    "eval_results = {}\n",
    "\n",
    "for mode in EVAL_CONFIG['models_to_evaluate']:\n",
    "    model_path = f'models/ppo_{mode}.pth'\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Skipping {mode}: model not found at {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating {mode}...\")\n",
    "    \n",
    "    # Create evaluation config\n",
    "    eval_cfg = EvaluationConfig.from_config(\n",
    "        mode=mode,\n",
    "        model_path=model_path,\n",
    "        total_timesteps=EVAL_CONFIG['total_timesteps'],\n",
    "        n_envs=EVAL_CONFIG['n_envs'],\n",
    "        deterministic=EVAL_CONFIG['deterministic'],\n",
    "        output_path=f'generated/eval_{mode}.json',\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    tester = ModelTester(eval_cfg)\n",
    "    result = tester.run()\n",
    "    eval_results[mode] = result\n",
    "    \n",
    "    # Clean up\n",
    "    del tester\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "if len(eval_results) >= 1:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Build comparison dataframe from aggregates\n",
    "    comparison = []\n",
    "    for mode, result in eval_results.items():\n",
    "        agg = result.get('aggregates')\n",
    "        if agg:\n",
    "            comparison.append({\n",
    "                'Mode': mode,\n",
    "                'Deaths': agg['n_deaths'],\n",
    "                'Survival (mean)': f\"{agg['survival_mean']:.0f}\",\n",
    "                'Food/Death': f\"{agg['food_per_death_mean']:.1f}\",\n",
    "                'Poison/Death': f\"{agg['poison_per_death_mean']:.1f}\",\n",
    "                'Efficiency': f\"{agg['overall_efficiency']:.1%}\",\n",
    "                'Deaths/1k Steps': f\"{agg['deaths_per_1k_steps']:.2f}\",\n",
    "            })\n",
    "    \n",
    "    if comparison:\n",
    "        df = pd.DataFrame(comparison)\n",
    "        print(\"\\nComparison of trained agents:\")\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goodhart's Law demonstration\n",
    "if 'ground_truth' in eval_results and 'proxy' in eval_results:\n",
    "    gt_agg = eval_results['ground_truth'].get('aggregates')\n",
    "    px_agg = eval_results['proxy'].get('aggregates')\n",
    "    \n",
    "    if gt_agg and px_agg:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GOODHART'S LAW DEMONSTRATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        survival_diff = gt_agg['survival_mean'] - px_agg['survival_mean']\n",
    "        efficiency_diff = gt_agg['overall_efficiency'] - px_agg['overall_efficiency']\n",
    "        poison_diff = px_agg['poison_per_death_mean'] - gt_agg['poison_per_death_mean']\n",
    "        \n",
    "        print(f\"\\nThe proxy agent optimizes for 'interestingness' instead of energy.\")\n",
    "        print(f\"\")\n",
    "        print(f\"Results:\")\n",
    "        print(f\"  Survival gap: {survival_diff:+.0f} steps (ground truth lives longer)\")\n",
    "        print(f\"  Efficiency gap: {efficiency_diff:+.1%} (ground truth finds more food)\")\n",
    "        print(f\"  Poison eaten: proxy eats {poison_diff:.1f} more per death\")\n",
    "        print(f\"\")\n",
    "        print(f\"The proxy metric (interestingness) failed as a target.\")\n",
    "        print(f\"When the agent optimized for it, it ate MORE poison because\")\n",
    "        print(f\"poison is configured to be MORE interesting than food.\")\n",
    "        print(f\"\")\n",
    "        print(f\"This is Goodhart's Law in action: optimizing for a proxy metric\")\n",
    "        print(f\"(interestingness) led to worse outcomes on the true objective (survival).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualization\n",
    "\n",
    "Plot evaluation results and compare agent behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot survival comparison\n",
    "if len(eval_results) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    modes = list(eval_results.keys())\n",
    "    colors = ['#16c79a', '#ff6b6b', '#ffa500', '#00d9ff']\n",
    "    \n",
    "    # 1. Survival time comparison\n",
    "    ax = axes[0]\n",
    "    survivals = [eval_results[m]['aggregates']['survival_mean'] for m in modes if eval_results[m].get('aggregates')]\n",
    "    survival_stds = [eval_results[m]['aggregates']['survival_std'] for m in modes if eval_results[m].get('aggregates')]\n",
    "    valid_modes = [m for m in modes if eval_results[m].get('aggregates')]\n",
    "    \n",
    "    bars = ax.bar(valid_modes, survivals, color=colors[:len(valid_modes)], yerr=survival_stds, capsize=5)\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('Mean Survival Time')\n",
    "    ax.set_ylim(0, max(survivals) * 1.3)\n",
    "    \n",
    "    # 2. Efficiency comparison\n",
    "    ax = axes[1]\n",
    "    efficiencies = [eval_results[m]['aggregates']['overall_efficiency'] * 100 for m in valid_modes]\n",
    "    \n",
    "    bars = ax.bar(valid_modes, efficiencies, color=colors[:len(valid_modes)])\n",
    "    ax.set_ylabel('Efficiency (%)')\n",
    "    ax.set_title('Food Efficiency (food / total consumed)')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 3. Consumption comparison\n",
    "    ax = axes[2]\n",
    "    food_rates = [eval_results[m]['aggregates']['food_per_1k_steps'] for m in valid_modes]\n",
    "    poison_rates = [eval_results[m]['aggregates']['poison_per_1k_steps'] for m in valid_modes]\n",
    "    \n",
    "    x = np.arange(len(valid_modes))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, food_rates, width, label='Food', color='#16c79a')\n",
    "    ax.bar(x + width/2, poison_rates, width, label='Poison', color='#ff6b6b')\n",
    "    ax.set_ylabel('Per 1000 Steps')\n",
    "    ax.set_title('Consumption Rates')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(valid_modes)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 evaluated models for comparison plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival time distributions\n",
    "if len(eval_results) >= 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    for mode, result in eval_results.items():\n",
    "        deaths = result.get('deaths', [])\n",
    "        if deaths:\n",
    "            survival_times = [d['survival_time'] for d in deaths]\n",
    "            ax.hist(survival_times, bins=30, alpha=0.6, label=f'{mode} (n={len(deaths)})')\n",
    "    \n",
    "    ax.set_xlabel('Survival Time (steps)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Survival Time Distribution')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Download Models\n",
    "\n",
    "Download trained models to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a specific model\n",
    "from google.colab import files\n",
    "\n",
    "# Uncomment to download:\n",
    "# files.download('models/ppo_ground_truth.pth')\n",
    "# files.download('models/ppo_proxy.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "### Training Tips\n",
    "- **Memory errors (OOM):** Reduce `n_envs` or increase `n_minibatches`\n",
    "- **Slow training:** Enable `compile_models` (slower startup, faster training)\n",
    "- **Better results:** Increase `total_timesteps` to 1M+\n",
    "\n",
    "### Understanding the Modes\n",
    "- **ground_truth:** Agent sees actual cell types (food vs poison). Should learn to survive.\n",
    "- **proxy:** Agent sees only \"interestingness\" values. Will learn to seek poison (more interesting than food).\n",
    "- **ground_truth_handhold:** Ground truth with scaled rewards. Easier learning.\n",
    "- **ground_truth_blinded:** Proxy observations but real energy reward. Control condition.\n",
    "\n",
    "### The Goodhart Demonstration\n",
    "When you train both `ground_truth` and `proxy` modes:\n",
    "1. Ground truth agent learns to eat food and avoid poison (survives longer)\n",
    "2. Proxy agent optimizes for interestingness (eats poison, dies faster)\n",
    "\n",
    "This demonstrates Goodhart's Law: the proxy metric (interestingness) fails catastrophically when optimized directly.\n",
    "\n",
    "### Key Metrics\n",
    "- **Survival time:** How long agents live before dying (starvation)\n",
    "- **Efficiency:** food / (food + poison) - the key Goodhart metric\n",
    "- **Deaths per 1k steps:** Population death rate\n",
    "\n",
    "### TensorBoard Metrics\n",
    "- **loss/policy, loss/value:** PPO training losses (should decrease)\n",
    "- **metrics/entropy:** Policy entropy (should decrease as policy becomes confident)\n",
    "- **metrics/food_ratio:** Curriculum-invariant efficiency metric\n",
    "- **reward/episode:** Episode rewards (mode-specific, affected by curriculum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
